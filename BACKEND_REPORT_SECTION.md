Backend Architecture and Security

This project uses a split backend design to meet two competing goals: rapid development of the clinical workflow and secure handling of sensitive wound images. Firebase is used for user identity and lightweight metadata, while AWS provides encrypted object storage and the compute necessary to run large PyTorch models reliably. This division keeps the client experience simple while ensuring that heavy inference workloads and encrypted data handling are isolated in the AWS layer.

On the client side, authentication is handled by Firebase Auth. The app never stores personal health information in Firestore; instead, it stores only non-identifying metadata required to locate and decrypt image artifacts. Firestore documents contain fields such as the S3 object key, an encrypted data key, the AES-GCM IV, and minimal analysis context. This design allows a clear audit path without exposing patient identifiers in the database layer.

Image handling is secured through client-side encryption combined with server-side encryption at rest. When the app needs to upload an image, it calls an API Gateway endpoint (/upload-init) backed by AWS Lambda. Lambda generates a KMS data key and a pre-signed S3 PUT URL. The data key is returned to the client in two forms: plaintext for immediate encryption and ciphertext for later decryption on the server. The client encrypts the image using AES-256-GCM and uploads the ciphertext to S3 via the pre-signed URL. The plaintext key is never stored. S3 enforces TLS and SSE-KMS through a bucket policy, so objects are encrypted in transit and at rest.

Inference is served by a FastAPI application running on an EC2 instance. The models are too large for Lambda in both size and memory constraints, and they benefit from being loaded once and reused across requests. The EC2 service loads a segmentation model and a two-model classification ensemble at startup. When the app requests /infer, it sends only the S3 key and the encrypted key metadata. The server downloads the ciphertext from S3, uses KMS to decrypt the data key, and then decrypts the image locally using AES-GCM. The resulting image is passed through the segmentation pipeline to compute wound area and ROI, and then through the ensemble to produce stage probabilities and a final stage prediction. The response includes the stage label, confidence, probability distribution, wound area metrics, and a segmentation mask.

Access to inference is protected with Firebase ID tokens. The Flutter client attaches a Bearer token to /infer requests. The EC2 server verifies the token using Firebase Admin SDK credentials. This prevents unauthenticated calls to the inference endpoint and ties the inference request to a Firebase identity without exposing long-lived credentials in the client. A secondary endpoint (/download-url) provides time-limited pre-signed GET access for authorized users who need to view stored images, keeping the bucket private.

The trade-off in this design is operational overhead. EC2 requires instance management and monitoring, and HTTP is used in the lab environment for simplicity. In a production deployment, the same architecture can be moved behind HTTPS using an ALB and ACM certificates, and autoscaling can be introduced if throughput increases. However, for the project scope, the chosen architecture provides a strong balance of security, practicality, and inference reliability.
